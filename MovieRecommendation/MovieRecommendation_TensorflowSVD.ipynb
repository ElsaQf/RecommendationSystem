{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    }
   ],
   "source": [
    "%autosave 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.TensorFlow with SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](svd_recommendation.png)\n",
    "LFM:把用户在item上打分的行为，看作是有内部依据的，认为和k个factor有关<br>\n",
    "每一个user i会有一个user的向量（k维），每一个item会有一个item的向量（k维）\n",
    "### 预测公式\n",
    "$y_{pred[u, i]} = bias_{global} + bias_{user[u]} + bias_{item_[i]} + <embedding_{user[u]}, embedding_{item[i]}>$\n",
    "### 最小化loss（添加正则化项）\n",
    "$\\sum_{u, i} |y_{pred[u, i]} - y_{true[u, i]}|^2 + \\lambda(|embedding_{user[u]}|^2 + |embedding_{item[i]}|^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.获取数据\n",
    "数据集movielens, 数据格式**suer item rating timestamp**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.数据处理\n",
    "- batch数据\n",
    "- shuffle用于训练train\n",
    "- 顺序用于测试test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T01:27:52.441847Z",
     "start_time": "2019-10-06T01:27:52.214179Z"
    }
   },
   "outputs": [],
   "source": [
    "# from __future__ import absolute_import, division, print_function\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T01:27:56.400013Z",
     "start_time": "2019-10-06T01:27:56.383923Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_data_and_process(filname, sep=\"\\t\"):\n",
    "    col_names = [\"user\", \"item\", \"rate\", \"st\"]\n",
    "    df = pd.read_csv(filname, sep=sep, header=None, names=col_names, engine='python')\n",
    "    df[\"user\"] -= 1\n",
    "    df[\"item\"] -= 1\n",
    "    for col in (\"user\", \"item\"):\n",
    "        df[col] = df[col].astype(np.int32)\n",
    "    df[\"rate\"] = df[\"rate\"].astype(np.float32)\n",
    "    return df\n",
    "\n",
    "\n",
    "class ShuffleDataIterator(object):\n",
    "    \"\"\"\n",
    "    随机生成一个batch一个batch数据\n",
    "    \"\"\"\n",
    "    #初始化\n",
    "    def __init__(self, inputs, batch_size=10):\n",
    "        self.inputs = inputs\n",
    "        self.batch_size = batch_size\n",
    "        self.num_cols = len(self.inputs)\n",
    "        self.len = len(self.inputs[0])\n",
    "        self.inputs = np.transpose(np.vstack([np.array(self.inputs[i]) for i in range(self.num_cols)]))\n",
    "\n",
    "    #总样本量\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    #取出下一个batch\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "    \n",
    "    #随机生成batch_size个下标，取出对应的样本\n",
    "    def next(self):\n",
    "        ids = np.random.randint(0, self.len, (self.batch_size,))\n",
    "        out = self.inputs[ids, :]\n",
    "        return [out[:, i] for i in range(self.num_cols)]\n",
    "\n",
    "\n",
    "class OneEpochDataIterator(ShuffleDataIterator):\n",
    "    \"\"\"\n",
    "    顺序产出一个epoch的数据，在测试中可能会用到\n",
    "    \"\"\"\n",
    "    def __init__(self, inputs, batch_size=10):\n",
    "        super(OneEpochDataIterator, self).__init__(inputs, batch_size=batch_size)\n",
    "        if batch_size > 0:\n",
    "            self.idx_group = np.array_split(np.arange(self.len), np.ceil(self.len / batch_size))\n",
    "        else:\n",
    "            self.idx_group = [np.arange(self.len)]\n",
    "        self.group_id = 0\n",
    "\n",
    "    def next(self):\n",
    "        if self.group_id >= len(self.idx_group):\n",
    "            self.group_id = 0\n",
    "            raise StopIteration\n",
    "        out = self.inputs[self.idx_group[self.group_id], :]\n",
    "        self.group_id += 1\n",
    "        return [out[:, i] for i in range(self.num_cols)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T01:28:00.551746Z",
     "start_time": "2019-10-06T01:28:00.536137Z"
    }
   },
   "outputs": [],
   "source": [
    "# 使用矩阵分解搭建的网络结构\n",
    "def inference_svd(user_batch, item_batch, user_num, item_num, dim=5, device=\"/cpu:0\"):\n",
    "    #使用CPU\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        # 初始化几个bias项\n",
    "        global_bias = tf.get_variable(\"global_bias\", shape=[])\n",
    "        w_bias_user = tf.get_variable(\"embd_bias_user\", shape=[user_num])\n",
    "        w_bias_item = tf.get_variable(\"embd_bias_item\", shape=[item_num])\n",
    "        # bias向量\n",
    "        bias_user = tf.nn.embedding_lookup(w_bias_user, user_batch, name=\"bias_user\")\n",
    "        bias_item = tf.nn.embedding_lookup(w_bias_item, item_batch, name=\"bias_item\")\n",
    "        w_user = tf.get_variable(\"embd_user\", shape=[user_num, dim],\n",
    "                                 initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        w_item = tf.get_variable(\"embd_item\", shape=[item_num, dim],\n",
    "                                 initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        # user向量与item向量\n",
    "        embd_user = tf.nn.embedding_lookup(w_user, user_batch, name=\"embedding_user\")\n",
    "        embd_item = tf.nn.embedding_lookup(w_item, item_batch, name=\"embedding_item\")\n",
    "    with tf.device(device):\n",
    "        # 按照实际公式进行计算\n",
    "        # 先对user向量和item向量求内积\n",
    "        infer = tf.reduce_sum(tf.multiply(embd_user, embd_item), 1)\n",
    "        # 加上几个偏置项\n",
    "        infer = tf.add(infer, global_bias)\n",
    "        infer = tf.add(infer, bias_user)\n",
    "        infer = tf.add(infer, bias_item, name=\"svd_inference\")\n",
    "        # 加上正则化项\n",
    "        regularizer = tf.add(tf.nn.l2_loss(embd_user), tf.nn.l2_loss(embd_item), name=\"svd_regularizer\")\n",
    "    return infer, regularizer\n",
    "\n",
    "# 迭代优化部分\n",
    "def optimization(infer, regularizer, rate_batch, learning_rate=0.001, reg=0.1, device=\"/cpu:0\"):\n",
    "    global_step = tf.train.get_global_step()\n",
    "    assert global_step is not None\n",
    "    # 选择合适的optimizer做优化\n",
    "    with tf.device(device):\n",
    "        cost_l2 = tf.nn.l2_loss(tf.subtract(infer, rate_batch))\n",
    "        penalty = tf.constant(reg, dtype=tf.float32, shape=[], name=\"l2\")\n",
    "        cost = tf.add(cost_l2, tf.multiply(regularizer, penalty))\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate).minimize(cost, global_step=global_step)\n",
    "    return cost, train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.模型搭建\n",
    "tensorflow搭建一个可增量训练的矩阵分解模型SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们按照下图的方式用tensorflow去搭建一个可增量训练的矩阵分解模型，完成基于矩阵分解的推荐系统。\n",
    "![](tf_svd_graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T01:28:07.184848Z",
     "start_time": "2019-10-06T01:28:06.235322Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T01:28:29.703159Z",
     "start_time": "2019-10-06T01:28:29.679189Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six import next\n",
    "from tensorflow.core.framework import summary_pb2\n",
    "\n",
    "np.random.seed(13575)\n",
    "\n",
    "# 一批数据的大小\n",
    "BATCH_SIZE = 2000\n",
    "# 用户数\n",
    "USER_NUM = 6040\n",
    "# 电影数\n",
    "ITEM_NUM = 3952\n",
    "# factor维度\n",
    "DIM = 15\n",
    "# 最大迭代轮数\n",
    "EPOCH_MAX = 200\n",
    "# 使用cpu做训练\n",
    "DEVICE = \"/cpu:0\"\n",
    "\n",
    "# 截断\n",
    "def clip(x):\n",
    "    return np.clip(x, 1.0, 5.0)\n",
    "\n",
    "# 这个是方便Tensorboard可视化做的summary\n",
    "def make_scalar_summary(name, val):\n",
    "    return summary_pb2.Summary(value=[summary_pb2.Summary.Value(tag=name, simple_value=val)])\n",
    "\n",
    "# 调用上面的函数获取数据\n",
    "def get_data():\n",
    "    df = read_data_and_process(\"ratings.dat\", sep=\"::\")\n",
    "    rows = len(df)\n",
    "    df = df.iloc[np.random.permutation(rows)].reset_index(drop=True)\n",
    "    split_index = int(rows * 0.9)\n",
    "    df_train = df[0:split_index]\n",
    "    df_test = df[split_index:].reset_index(drop=True)\n",
    "    print(df_train.shape, df_test.shape)\n",
    "    return df_train, df_test\n",
    "\n",
    "# 实际训练过程\n",
    "def svd(train, test):\n",
    "    samples_per_batch = len(train) // BATCH_SIZE\n",
    "\n",
    "    # 一批一批数据用于训练\n",
    "    iter_train = ShuffleDataIterator([train[\"user\"],\n",
    "                                         train[\"item\"],\n",
    "                                         train[\"rate\"]],\n",
    "                                        batch_size=BATCH_SIZE)\n",
    "    # 测试数据\n",
    "    iter_test = OneEpochDataIterator([test[\"user\"],\n",
    "                                         test[\"item\"],\n",
    "                                         test[\"rate\"]],\n",
    "                                        batch_size=-1)\n",
    "    # user和item batch\n",
    "    user_batch = tf.placeholder(tf.int32, shape=[None], name=\"id_user\")\n",
    "    item_batch = tf.placeholder(tf.int32, shape=[None], name=\"id_item\")\n",
    "    rate_batch = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "    # 构建graph和训练\n",
    "    infer, regularizer = inference_svd(user_batch, item_batch, user_num=USER_NUM, item_num=ITEM_NUM, dim=DIM,\n",
    "                                           device=DEVICE)\n",
    "    global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "    _, train_op = optimization(infer, regularizer, rate_batch, learning_rate=0.001, reg=0.05, device=DEVICE)\n",
    "\n",
    "    # 初始化所有变量\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    # 开始迭代\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        summary_writer = tf.summary.FileWriter(logdir=\"/tmp/svd/log\", graph=sess.graph)\n",
    "        print(\"{} {} {} {}\".format(\"epoch\", \"train_error\", \"val_error\", \"elapsed_time\"))\n",
    "        errors = deque(maxlen=samples_per_batch)\n",
    "        start = time.time()\n",
    "        for i in range(EPOCH_MAX * samples_per_batch):\n",
    "            users, items, rates = next(iter_train)\n",
    "            _, pred_batch = sess.run([train_op, infer], feed_dict={user_batch: users,\n",
    "                                                                   item_batch: items,\n",
    "                                                                   rate_batch: rates})\n",
    "            pred_batch = clip(pred_batch)\n",
    "            errors.append(np.power(pred_batch - rates, 2))\n",
    "            if i % samples_per_batch == 0:\n",
    "                train_err = np.sqrt(np.mean(errors))\n",
    "                test_err2 = np.array([])\n",
    "                for users, items, rates in iter_test:\n",
    "                    pred_batch = sess.run(infer, feed_dict={user_batch: users,\n",
    "                                                            item_batch: items})\n",
    "                    pred_batch = clip(pred_batch)\n",
    "                    test_err2 = np.append(test_err2, np.power(pred_batch - rates, 2))\n",
    "                end = time.time()\n",
    "                test_err = np.sqrt(np.mean(test_err2))\n",
    "                print(\"{:3d} {:f} {:f} {:f}(s)\".format(i // samples_per_batch, train_err, test_err,\n",
    "                                                       end - start))\n",
    "                train_err_summary = make_scalar_summary(\"training_error\", train_err)\n",
    "                test_err_summary = make_scalar_summary(\"test_error\", test_err)\n",
    "                summary_writer.add_summary(train_err_summary, i)\n",
    "                summary_writer.add_summary(test_err_summary, i)\n",
    "                start = end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T01:28:37.003637Z",
     "start_time": "2019-10-06T01:28:33.845185Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900188, 4) (100021, 4)\n"
     ]
    }
   ],
   "source": [
    "# 获取数据\n",
    "df_train, df_test = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-06T01:34:32.902633Z",
     "start_time": "2019-10-06T01:28:39.491059Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1006 09:28:39.527252 140259732391744 deprecation.py:506] From /opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1006 09:28:40.016280 140259732391744 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W1006 09:28:40.017280 140259732391744 deprecation.py:323] From <ipython-input-7-1230e01f748f>:65: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.get_or_create_global_step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch train_error val_error elapsed_time\n",
      "  0 2.809092 2.814245 0.096770(s)\n",
      "  1 2.647766 1.830043 1.728847(s)\n",
      "  2 1.298248 1.056412 1.770557(s)\n",
      "  3 0.987011 0.957294 1.727047(s)\n",
      "  4 0.932584 0.929845 1.753838(s)\n",
      "  5 0.917552 0.920138 1.759520(s)\n",
      "  6 0.909307 0.916079 1.762440(s)\n",
      "  7 0.907277 0.913769 1.769709(s)\n",
      "  8 0.904168 0.911504 1.764282(s)\n",
      "  9 0.899851 0.909579 1.745221(s)\n",
      " 10 0.897996 0.907440 1.761696(s)\n",
      " 11 0.896385 0.905048 1.753508(s)\n",
      " 12 0.893484 0.901885 1.732205(s)\n",
      " 13 0.889538 0.899306 1.753876(s)\n",
      " 14 0.886001 0.896120 1.758024(s)\n",
      " 15 0.883615 0.893988 1.759104(s)\n",
      " 16 0.879428 0.891709 1.757067(s)\n",
      " 17 0.878400 0.889971 1.773216(s)\n",
      " 18 0.876684 0.888457 1.760789(s)\n",
      " 19 0.875388 0.886905 1.765570(s)\n",
      " 20 0.872054 0.886135 1.762475(s)\n",
      " 21 0.871036 0.884684 1.778867(s)\n",
      " 22 0.869817 0.883479 1.764868(s)\n",
      " 23 0.866603 0.881972 1.757534(s)\n",
      " 24 0.864774 0.880058 1.764917(s)\n",
      " 25 0.863440 0.878467 1.752664(s)\n",
      " 26 0.859810 0.876528 1.727537(s)\n",
      " 27 0.856528 0.874796 1.762953(s)\n",
      " 28 0.852411 0.872472 1.766915(s)\n",
      " 29 0.849729 0.870230 1.772587(s)\n",
      " 30 0.844399 0.868476 1.753158(s)\n",
      " 31 0.840929 0.865908 1.747794(s)\n",
      " 32 0.834660 0.863602 1.765438(s)\n",
      " 33 0.831887 0.861742 1.765929(s)\n",
      " 34 0.828024 0.859972 1.758726(s)\n",
      " 35 0.824420 0.858037 1.756968(s)\n",
      " 36 0.820202 0.856380 1.766884(s)\n",
      " 37 0.815488 0.854532 1.756016(s)\n",
      " 38 0.811695 0.853102 1.760604(s)\n",
      " 39 0.808025 0.851729 1.754677(s)\n",
      " 40 0.804085 0.850473 1.761345(s)\n",
      " 41 0.799844 0.849269 1.754910(s)\n",
      " 42 0.795860 0.848444 1.762565(s)\n",
      " 43 0.793457 0.847915 1.750142(s)\n",
      " 44 0.790963 0.847314 1.767121(s)\n",
      " 45 0.786761 0.846660 1.776730(s)\n",
      " 46 0.785186 0.846353 1.768013(s)\n",
      " 47 0.781874 0.845876 1.727865(s)\n",
      " 48 0.780036 0.845306 1.762956(s)\n",
      " 49 0.777444 0.845136 1.763565(s)\n",
      " 50 0.775171 0.844896 1.751744(s)\n",
      " 51 0.772707 0.844953 1.771794(s)\n",
      " 52 0.771180 0.844949 1.764004(s)\n",
      " 53 0.769144 0.845102 1.772421(s)\n",
      " 54 0.768272 0.845131 1.718574(s)\n",
      " 55 0.766102 0.845080 1.746447(s)\n",
      " 56 0.764992 0.845047 1.760842(s)\n",
      " 57 0.763798 0.845370 1.762552(s)\n",
      " 58 0.762583 0.845372 1.763758(s)\n",
      " 59 0.762846 0.845255 1.753743(s)\n",
      " 60 0.759430 0.845327 1.758672(s)\n",
      " 61 0.759130 0.845576 1.760902(s)\n",
      " 62 0.758899 0.845368 1.754007(s)\n",
      " 63 0.756900 0.845630 1.763000(s)\n",
      " 64 0.757510 0.845597 1.742811(s)\n",
      " 65 0.756647 0.845912 1.751015(s)\n",
      " 66 0.754624 0.845866 1.757953(s)\n",
      " 67 0.755241 0.846298 1.772911(s)\n",
      " 68 0.754132 0.846312 1.762341(s)\n",
      " 69 0.753772 0.846255 1.771435(s)\n",
      " 70 0.752670 0.846324 1.762337(s)\n",
      " 71 0.753403 0.846226 1.758609(s)\n",
      " 72 0.751844 0.846167 1.765571(s)\n",
      " 73 0.752342 0.846310 1.758298(s)\n",
      " 74 0.751317 0.846467 1.769836(s)\n",
      " 75 0.751438 0.846807 1.763406(s)\n",
      " 76 0.751533 0.846992 1.761119(s)\n",
      " 77 0.750063 0.847162 1.769726(s)\n",
      " 78 0.751307 0.847074 1.766223(s)\n",
      " 79 0.750508 0.847049 1.771774(s)\n",
      " 80 0.749348 0.847157 1.751490(s)\n",
      " 81 0.748568 0.847472 1.748652(s)\n",
      " 82 0.748315 0.847696 1.760928(s)\n",
      " 83 0.749559 0.847619 1.754676(s)\n",
      " 84 0.748607 0.847771 1.762733(s)\n",
      " 85 0.746978 0.847941 1.754867(s)\n",
      " 86 0.747825 0.847934 1.770562(s)\n",
      " 87 0.747201 0.847842 1.749052(s)\n",
      " 88 0.747294 0.847892 1.766135(s)\n",
      " 89 0.748338 0.848083 1.756553(s)\n",
      " 90 0.746264 0.847799 1.769204(s)\n",
      " 91 0.747097 0.847869 1.762460(s)\n",
      " 92 0.747049 0.847966 1.775285(s)\n",
      " 93 0.746366 0.848027 1.761062(s)\n",
      " 94 0.746461 0.848004 1.708873(s)\n",
      " 95 0.746878 0.847971 1.766405(s)\n",
      " 96 0.745085 0.847645 1.766187(s)\n",
      " 97 0.745455 0.847906 1.759719(s)\n",
      " 98 0.744956 0.848116 1.761066(s)\n",
      " 99 0.745169 0.848058 1.750942(s)\n",
      "100 0.745400 0.848081 1.758398(s)\n",
      "101 0.744831 0.848155 1.765002(s)\n",
      "102 0.745018 0.848160 1.761256(s)\n",
      "103 0.746223 0.848252 1.769865(s)\n",
      "104 0.745275 0.848441 1.761871(s)\n",
      "105 0.744577 0.848477 1.755023(s)\n",
      "106 0.745068 0.848310 1.766575(s)\n",
      "107 0.745105 0.848261 1.756831(s)\n",
      "108 0.746220 0.848340 1.773379(s)\n",
      "109 0.744793 0.848451 1.764923(s)\n",
      "110 0.745246 0.848592 1.766156(s)\n",
      "111 0.743338 0.848844 1.763660(s)\n",
      "112 0.744098 0.848953 1.766065(s)\n",
      "113 0.744065 0.848853 1.767978(s)\n",
      "114 0.744642 0.848754 1.760028(s)\n",
      "115 0.743658 0.848795 1.748441(s)\n",
      "116 0.743833 0.848926 1.760773(s)\n",
      "117 0.744427 0.848864 1.765522(s)\n",
      "118 0.742673 0.848774 1.776243(s)\n",
      "119 0.744167 0.848841 1.761874(s)\n",
      "120 0.744279 0.848804 1.770190(s)\n",
      "121 0.743308 0.848710 1.757921(s)\n",
      "122 0.744430 0.848834 1.767811(s)\n",
      "123 0.743749 0.848931 1.752900(s)\n",
      "124 0.743783 0.848834 1.761648(s)\n",
      "125 0.743396 0.849140 1.761330(s)\n",
      "126 0.744383 0.848794 1.767538(s)\n",
      "127 0.742482 0.848736 1.765824(s)\n",
      "128 0.743111 0.848843 1.765837(s)\n",
      "129 0.743057 0.848929 1.767501(s)\n",
      "130 0.743538 0.848942 1.751828(s)\n",
      "131 0.742606 0.849070 1.764218(s)\n",
      "132 0.743344 0.848787 1.760509(s)\n",
      "133 0.742912 0.848902 1.763822(s)\n",
      "134 0.743751 0.848844 1.756627(s)\n",
      "135 0.743304 0.849062 1.774131(s)\n",
      "136 0.742575 0.849160 1.756303(s)\n",
      "137 0.742334 0.849222 1.753949(s)\n",
      "138 0.743591 0.849467 1.754684(s)\n",
      "139 0.742734 0.849469 1.756494(s)\n",
      "140 0.742698 0.849518 1.761069(s)\n",
      "141 0.743832 0.849604 1.762047(s)\n",
      "142 0.742862 0.849521 1.764869(s)\n",
      "143 0.742567 0.849396 1.754997(s)\n",
      "144 0.744108 0.849230 1.764550(s)\n",
      "145 0.741796 0.849313 1.768570(s)\n",
      "146 0.742622 0.849461 1.758929(s)\n",
      "147 0.742115 0.849352 1.758743(s)\n",
      "148 0.742190 0.849580 1.743512(s)\n",
      "149 0.743003 0.849471 1.766818(s)\n",
      "150 0.743153 0.849517 1.763210(s)\n",
      "151 0.742831 0.849519 1.763173(s)\n",
      "152 0.742860 0.849283 1.725743(s)\n",
      "153 0.742085 0.849179 1.757030(s)\n",
      "154 0.743108 0.849262 1.766878(s)\n",
      "155 0.741355 0.849355 1.767179(s)\n",
      "156 0.742485 0.849537 1.754331(s)\n",
      "157 0.742745 0.849401 1.742197(s)\n",
      "158 0.741310 0.849469 1.764956(s)\n",
      "159 0.742856 0.849111 1.751201(s)\n",
      "160 0.742682 0.849344 1.764396(s)\n",
      "161 0.742685 0.849285 1.763216(s)\n",
      "162 0.741389 0.849376 1.705576(s)\n",
      "163 0.742192 0.849432 1.760241(s)\n",
      "164 0.741517 0.849293 1.769231(s)\n",
      "165 0.741853 0.849481 1.764103(s)\n",
      "166 0.743489 0.849596 1.763127(s)\n",
      "167 0.742188 0.849545 1.749640(s)\n",
      "168 0.741023 0.849455 1.753155(s)\n",
      "169 0.742629 0.849332 1.752828(s)\n",
      "170 0.742456 0.849354 1.755694(s)\n",
      "171 0.741877 0.849273 1.766206(s)\n",
      "172 0.742394 0.849396 1.771504(s)\n",
      "173 0.741681 0.849257 1.760725(s)\n",
      "174 0.741708 0.849249 1.771803(s)\n",
      "175 0.741332 0.849170 1.773526(s)\n",
      "176 0.741308 0.849324 1.770514(s)\n",
      "177 0.742390 0.849395 1.765293(s)\n",
      "178 0.741682 0.849275 1.759170(s)\n",
      "179 0.741816 0.849397 1.770112(s)\n",
      "180 0.742152 0.849562 1.763174(s)\n",
      "181 0.741256 0.849647 1.760202(s)\n",
      "182 0.741561 0.849638 1.761207(s)\n",
      "183 0.742320 0.849880 1.746294(s)\n",
      "184 0.742051 0.849835 1.762702(s)\n",
      "185 0.741406 0.849804 1.770882(s)\n",
      "186 0.743392 0.849957 1.780668(s)\n",
      "187 0.742502 0.849877 1.766610(s)\n",
      "188 0.741707 0.849781 1.776471(s)\n",
      "189 0.741778 0.849508 1.763687(s)\n",
      "190 0.741915 0.849641 1.764997(s)\n",
      "191 0.740590 0.849668 1.764295(s)\n",
      "192 0.741730 0.849912 1.770427(s)\n",
      "193 0.741358 0.849626 1.763920(s)\n",
      "194 0.742227 0.849548 1.777086(s)\n",
      "195 0.742109 0.849624 1.761916(s)\n",
      "196 0.742512 0.849730 1.764841(s)\n",
      "197 0.741577 0.849602 1.765574(s)\n",
      "198 0.740538 0.849728 1.766357(s)\n",
      "199 0.740953 0.849735 1.759635(s)\n"
     ]
    }
   ],
   "source": [
    "# 完成实际的训练\n",
    "svd(df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
